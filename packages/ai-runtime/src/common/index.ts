import { ImageModelInfo, ServiceMode, ServiceModePermissionMap, TextModelInfo } from '../types/common';

export const defaultTextModel = 'gpt-4o-mini';

export async function getSupportedModels(): Promise<TextModelInfo[]> {
  return [
    {
      brand: 'OpenAI',
      model: 'gpt-4o',
      name: 'GPT4o',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 128000,
      maxTokensDefault: 128000,
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'gpt-4o-mini',
      name: 'GPT4o mini',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 128000,
      maxTokensDefault: 128000,
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'gpt-3.5-turbo',
      name: 'GPT3.5 turbo',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 4096,
      maxTokensDefault: 4096,
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'gpt-3.5-turbo-16k',
      name: 'GPT3.5 turbo 16k',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 16385,
      maxTokensDefault: 16385,
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'gpt-4',
      name: 'GPT4',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 8192,
      maxTokensDefault: 8192,
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'gpt-4-32k',
      name: 'GPT4 32k',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      presencePenaltyMin: -2,
      presencePenaltyMax: 2,
      presencePenaltyDefault: 0,
      frequencyPenaltyMin: -2,
      frequencyPenaltyMax: 2,
      frequencyPenaltyDefault: 0,
      maxTokensMin: 1,
      maxTokensMax: 32768,
      maxTokensDefault: 32768,
      tags: ['OpenAI'],
    },
    {
      brand: 'Google',
      model: 'gemini-pro',
      name: 'Gemini Pro',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0.1,
      topPMax: 1,
      topPDefault: 1,
      maxTokensMin: 1,
      maxTokensMax: 2048,
      maxTokensDefault: 2048,
      tags: ['Google'],
    },
    {
      brand: 'Mistral AI',
      model: 'openRouter/mistralai/mistral-7b-instruct',
      name: 'Mistral 7B Instruct',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      maxTokensMin: 1,
      maxTokensMax: 8192,
      maxTokensDefault: 8192,
      tags: ['Mistral AI'],
    },
    {
      brand: 'Mistral AI',
      model: 'openRouter/mistralai/mixtral-8x7b-instruct',
      name: 'Mixtral 8x7B Instruct (beta)',
      temperatureMin: 0,
      temperatureMax: 2,
      temperatureDefault: 1,
      topPMin: 0,
      topPMax: 1,
      topPDefault: 1,
      maxTokensMin: 1,
      maxTokensMax: 32768,
      maxTokensDefault: 32768,
      tags: ['Mistral AI'],
    },
  ];
}

export const defaultImageModel = 'dall-e-2';

export async function getSupportedImagesModels(): Promise<ImageModelInfo[]> {
  return [
    {
      brand: 'OpenAI',
      model: 'dall-e-2',
      nMin: 1,
      nMax: 10,
      nDefault: 1,
      size: ['256x256', '512x512', '1024x1024'],
      sizeDefault: '256x256',
      tags: ['OpenAI'],
    },
    {
      brand: 'OpenAI',
      model: 'dall-e-3',
      nMin: 1,
      nMax: 1,
      nDefault: 1,
      quality: ['standard', 'hd'],
      qualityDefault: 'standard',
      size: ['1024x1024', '1792x1024', '1024x1792'],
      sizeDefault: '1024x1024',
      style: ['vivid', 'natural'],
      styleDefault: 'vivid',
      tags: ['OpenAI'],
    },
  ];
}

export async function getModelBrand(model: string) {
  const modelsArray = await Promise.all([getSupportedModels(), getSupportedImagesModels()]);
  return modelsArray.flat().find((m) => m.model === model)?.brand || null;
}

export function getServiceModePermissionMap(serviceMode: ServiceMode): ServiceModePermissionMap {
  const permissionMap = {
    single: {
      ensureViewAllProjectsRoles: ['owner', 'admin', 'promptsEditor'],
      ensurePromptsEditorRoles: ['owner', 'admin', 'promptsEditor'],
      ensurePromptsAdminRoles: ['owner', 'admin', 'promptsEditor'],
    },
    multiple: {
      ensureViewAllProjectsRoles: [],
      // no need to check, everyone can do it, will check author permission in the backend
      ensurePromptsEditorRoles: undefined,
      ensurePromptsAdminRoles: ['owner', 'admin', 'promptsEditor'],
    },
  };

  // try to fallback to 'single-tenant'
  return permissionMap[serviceMode] || permissionMap.single;
}
